{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import IPython\n",
    "import pylab\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pylab as py\n",
    "import scipy as sp\n",
    "\n",
    "import scipy.signal\n",
    "\n",
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch_dsl import Search, Q\n",
    "from elasticsearch_dsl.connections import connections\n",
    "from datetime import datetime, timedelta\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "connections.create_connection(timeout=180)\n",
    "pylab.rcParams['figure.figsize'] = (16, 4)\n",
    "plt.hold(True)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "client = Elasticsearch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_iso_date(str):\n",
    "    return datetime.strptime(str, '%Y-%m-%dT%H:%M:%S.%fZ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hist_data(terms=None, start='1900-01-01', end='2020-01-01'):\n",
    "    q = Search(index='records-hep')[0:0]\n",
    "    if terms:\n",
    "        q = q.query('match', abstract=terms)\n",
    "    q = q.filter('range', earliest_date={'gte': start, 'lt': end})\n",
    "    q.aggs.bucket(\n",
    "        'group_by_date', \n",
    "        'date_histogram', \n",
    "        field='earliest_date', \n",
    "        interval='day',\n",
    "        format='date_optional_time'\n",
    "    )\n",
    "    return q.execute().aggregations.group_by_date.buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unzip(data):\n",
    "    if not len(data):\n",
    "        return [], []\n",
    "    x, y = zip(*[(parse_iso_date(e.key_as_string), e.doc_count) for e in data])\n",
    "    return np.array(x), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hist_data_precise(terms=None, start='1900-01-01', end='2020-01-01', show_query=False):\n",
    "    q = Search(index='hep-slim')[0:0] \\\n",
    "        .filter(\"script\", script=\"d = doc['earliest_date'].date; d.getDayOfMonth() != 1\") \\\n",
    "        .filter('range', earliest_date={'gte': start, 'lt': end})\n",
    "    q.aggs.bucket(\n",
    "        'group_by_date',\n",
    "        'date_histogram',\n",
    "        field='earliest_date',\n",
    "        interval='day',\n",
    "        format='date_optional_time'\n",
    "    )\n",
    "    if terms:\n",
    "        q = q.query('match', abstract=terms)\n",
    "    if show_query:\n",
    "        print(q.to_dict())\n",
    "    return q.execute().aggregations.group_by_date.buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reverse_date_references(show_query=False):\n",
    "    '''Builds a reverse index from recid to the minimum dates of papers referring this recid.'''\n",
    "    \n",
    "    q = Search(index='hep-slim')[0:0] \\\n",
    "        .filter(\"script\", script=\"d = doc['earliest_date'].date; d.getDayOfMonth() != 1\")\n",
    "    agg = q.aggs.bucket(\n",
    "        'references',\n",
    "        'terms',\n",
    "        field='references.recid',\n",
    "        size=0\n",
    "    )\n",
    "    agg.bucket('min_earliest_date', 'min', field='earliest_date')\n",
    "    agg.bucket('min_preprint_date', 'min', field='preprint_date')\n",
    "    agg.bucket('min_modification_date', 'min', field='creation_modification_date.modification_date')\n",
    "    agg.bucket('min_creation_date', 'min', field='creation_modification_date.creation_date')\n",
    "    if show_query:\n",
    "        print(q.to_dict())\n",
    "    res = q.execute().aggregations.references.buckets\n",
    "    \n",
    "    def extract_date(elem):\n",
    "        return parse_iso_date(elem.value_as_string) if \"value_as_string\" in elem else None\n",
    "    \n",
    "    mappings = {}\n",
    "    for elem in res:\n",
    "        mappings[elem.key] = {\n",
    "            'earliest_date': extract_date(elem.min_earliest_date),\n",
    "            'preprint_date': extract_date(elem.min_preprint_date), # may be missing\n",
    "            'modification_date': extract_date(elem.min_modification_date),\n",
    "            'creation_date': extract_date(elem.min_creation_date)\n",
    "        }\n",
    "    return mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def precise_recid(show_query=False):\n",
    "    '''Retrieves recid and precise earliest date.'''\n",
    "    \n",
    "    q = Search(index='hep-slim') \\\n",
    "        .fields(['self_recid', 'earliest_date']) \\\n",
    "        .filter(\"script\", script=\"d = doc['earliest_date'].date; d.getDayOfMonth() != 1\")\n",
    "    if show_query:\n",
    "        print(q.to_dict())\n",
    "    res = []\n",
    "    for hit in q.scan():\n",
    "        res.append((hit.self_recid[0], datetime.strptime(hit.earliest_date[0], '%Y-%m-%d')))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def hist_data_selected(terms=None, start='2013-02-01', end='2016-06-01', granularity='day'):\n",
    "    q = Search(index='hep-recent')[0:0] \\\n",
    "        .filter(\"script\", script=\"d = doc['earliest_date'].date; d.getDayOfMonth() != 1 || d.getMonthOfYear() != 1\")\n",
    "    if terms:\n",
    "        q = q.query('match', abstract=terms)\n",
    "    q = q.filter('range', earliest_date={'gte': start, 'lt': end})\n",
    "    q.aggs.bucket(\n",
    "        'group_by_date', \n",
    "        'date_histogram', \n",
    "        field='earliest_date', \n",
    "        interval=granularity,\n",
    "        format='date_optional_time'\n",
    "    )\n",
    "    return q.execute().aggregations.group_by_date.buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def interval_ids(date, days=7):\n",
    "    start_date = datetime.strptime(date, '%Y-%m-%d')\n",
    "    end_date = start_date + timedelta(days=days)\n",
    "    q = Search(index='hep-analysis') \\\n",
    "        .fields(['self_recid']) \\\n",
    "        .filter('range', earliest_date={'gte': start_date, 'lt': end_date}) \\\n",
    "        .filter('exists', field='abstracts.value')\n",
    "    \n",
    "    res = []\n",
    "    for e in q.scan():\n",
    "        res.append(e.meta.id)\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def term_vectors(ids, field, chunk=100):\n",
    "    res = []\n",
    "    for pos in range(0, len(ids), chunk):\n",
    "        q = client.mtermvectors(\n",
    "            index='hep-analysis', \n",
    "            doc_type='hep',\n",
    "            ids=ids[pos:pos + chunk],\n",
    "            fields=[field],\n",
    "            field_statistics=False,\n",
    "            term_statistics=True,\n",
    "            offsets=False,\n",
    "            payloads=False,\n",
    "            positions=False,\n",
    "            realtime=True\n",
    "        )\n",
    "        for e in q['docs']:\n",
    "            if field in e['term_vectors']:\n",
    "                res.append(e['term_vectors'][field]['terms'])\n",
    "    assert len(ids) == len(res)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def fold_vectors(vectors):\n",
    "    words = {}\n",
    "    for vec in vectors:\n",
    "        for word, freqs in vec.items():\n",
    "            if word in words:\n",
    "                #assert words[word]['term_total'] == freqs['ttf']\n",
    "                #assert words[word]['doc_total'] == freqs['doc_freq']\n",
    "                #words[word]['history'].append(freqs['doc_freq'])\n",
    "                words[word]['term_freq'] += freqs['term_freq']\n",
    "                words[word]['doc_freq'] += 1\n",
    "            else:\n",
    "                words[word] = {\n",
    "                    'term_total': freqs['ttf'], # estimate\n",
    "                    'doc_total': freqs['doc_freq'], # estimate\n",
    "                    'term_freq': freqs['term_freq'],\n",
    "                    'doc_freq': 1,\n",
    "                    #'history': [freqs['doc_freq']],\n",
    "                }\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def zero_div(a, b):\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        res = np.divide(a, b)\n",
    "        res[res == np.inf] = 0\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
